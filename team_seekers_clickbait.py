# -*- coding: utf-8 -*-
"""team_seekers_clickbait.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-8HBJH6af9clyOREuFLlXQdHfsbGAlmh
"""

# Commented out IPython magic to ensure Python compatibility.
class team_seekers_clickbait():

  def tokenization(text):
    lst=text.split()
    return lst

  def lowercasing(lst):
      new_lst=[]
      for i in lst:
          i=i.lower()
          new_lst.append(i)
      return new_lst
 
  def remove_punctuations(lst):
      new_lst=[]
      for i in lst:
          for j in s.punctuation:
              i=i.replace(j,'')
          new_lst.append(i)
      return new_lst


  def remove_numbers(lst):
      nodig_lst=[]
      new_lst=[]
      for i in lst:
          for j in s.digits:    
              i=i.replace(j,'')
          nodig_lst.append(i)
      for i in nodig_lst:
          if i!='':
              new_lst.append(i)
      return new_lst

  def remove_stopwords(lst):
      stop=stopwords.words('english')
      new_lst=[]
      for i in lst:
          if i not in stop:
              new_lst.append(i)
      return new_lst

  def predict(self,text):
    from sklearn.feature_extraction.text import TfidfVectorizer
    import cloudpickle
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
#     %matplotlib inline
    import gensim
    from gensim.utils import simple_preprocess
    from gensim.parsing.preprocessing import STOPWORDS
    from nltk.stem import WordNetLemmatizer, SnowballStemmer
    from gensim.models.word2vec import Word2Vec
    from gensim.models.doc2vec import TaggedDocument
    from sklearn.model_selection import train_test_split
    import warnings
    import numpy as np
    np.random.seed(2018)
    import nltk
    nltk.download('wordnet')
    import re
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    import nltk
    import pickle
    nltk.download('stopwords')
    from nltk.corpus import stopwords 
    from nltk.stem.porter import PorterStemmer
    from nltk.tokenize import word_tokenize 
    nltk.download('punkt')
    import nltk
    from scipy import sparse
    from urllib.request import urlopen

    nltk.download('averaged_perceptron_tagger')
    multinomial1 = cloudpickle.load(urlopen("https://github.com/Manishayacham/Alternus-Vera/blob/main/Multinomial_Naive_Bayes.sav?raw=true"))
    tfidf =  cloudpickle.load(urlopen("https://github.com/Manishayacham/Alternus-Vera/blob/main/tfidf.sav?raw=true"))
    dfrme = pd.DataFrame(index=[0], columns=['text'])
    dfrme['text'] = text
    predict=dfrme['text'].apply(tokenization)
    predict=predict.apply(lowercasing)
    predict=predict.apply(remove_punctuations)
    predict=predict.apply(remove_stopwords)
    predict=predict.apply(remove_spaces)
    predict=predict.apply(lemmatzation)
    predict =predict.apply(lambda x: ''.join(i+' ' for i in x))
    text = tfidf.transform(predict)
    train_arr=text.toarray()
    probValue = multinomial1.predict_proba(train_arr)[:,1][0]
    return probValue